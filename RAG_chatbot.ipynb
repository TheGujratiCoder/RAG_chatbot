{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604e1d69-89c7-4280-9b8d-74fb2ecc52ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:33:08.406300700Z",
     "start_time": "2024-06-08T10:31:25.596291400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c18b4903764ec8aa8a7526d3d8ac21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'c:/Users/Varad Acharya/Downloads/RAG/dataset/data-00000-of-00001.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 56\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave_to_disk(dataset_path)\n\u001b[0;32m     57\u001b[0m dataset\u001b[38;5;241m.\u001b[39madd_faiss_index(column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m dataset\u001b[38;5;241m.\u001b[39mget_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(index_path)\n",
      "File \u001b[1;32mc:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:1587\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[1;34m(self, dataset_path, fs, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m   1586\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m kwargs_per_job:\n\u001b[1;32m-> 1587\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_save_to_disk_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1588\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   1589\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:1611\u001b[0m, in \u001b[0;36mDataset._save_to_disk_single\u001b[1;34m(job_id, shard, fpath, storage_options)\u001b[0m\n\u001b[0;32m   1608\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mDEFAULT_MAX_BATCH_SIZE\n\u001b[0;32m   1610\u001b[0m num_examples_progress_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1611\u001b[0m writer \u001b[38;5;241m=\u001b[39m ArrowWriter(\n\u001b[0;32m   1612\u001b[0m     features\u001b[38;5;241m=\u001b[39mshard\u001b[38;5;241m.\u001b[39mfeatures,\n\u001b[0;32m   1613\u001b[0m     path\u001b[38;5;241m=\u001b[39mfpath,\n\u001b[0;32m   1614\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1615\u001b[0m     embed_local_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1616\u001b[0m )\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1618\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\datasets\\arrow_writer.py:338\u001b[0m, in \u001b[0;36mArrowWriter.__init__\u001b[1;34m(self, schema, features, path, stream, fingerprint, writer_batch_size, hash_salt, check_duplicates, disable_nullable, update_features, with_metadata, unit, embed_local_files, storage_options)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs: fsspec\u001b[38;5;241m.\u001b[39mAbstractFileSystem \u001b[38;5;241m=\u001b[39m fs\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39munstrip_protocol(path)\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closable_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\fsspec\\spec.py:1307\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1307\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[0;32m   1308\u001b[0m         path,\n\u001b[0;32m   1309\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   1310\u001b[0m         block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1311\u001b[0m         autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[0;32m   1312\u001b[0m         cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1313\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1314\u001b[0m     )\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mc:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:180\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:302\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n",
      "File \u001b[1;32mc:\\Users\\Varad Acharya\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:307\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    309\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'c:/Users/Varad Acharya/Downloads/RAG/dataset/data-00000-of-00001.arrow'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import streamlit as st\n",
    "\n",
    "# Function to read text file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=512):\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < chunk_size:\n",
    "            current_chunk += sentence + \".\"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \".\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Download the text file (Pride and Prejudice by Jane Austen)\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "txt_path = \"pride_and_prejudice.txt\"\n",
    "with open(txt_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract and preprocess text\n",
    "txt_path = \"pride_and_prejudice.txt\"\n",
    "raw_text = read_text_file(txt_path)\n",
    "chunks = split_text_into_chunks(raw_text)\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for the chunks\n",
    "embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "# Create a dataset from the chunks and embeddings\n",
    "data = {\"title\": [f\"Chunk {i}\" for i in range(len(chunks))], \"text\": chunks, \"embeddings\": embeddings.tolist()}\n",
    "features = Features({\"title\": Value(\"string\"), \"text\": Value(\"string\"), \"embeddings\": Sequence(Value(\"float32\"))})\n",
    "dataset = Dataset.from_dict(data, features=features)\n",
    "\n",
    "\n",
    "# Save the dataset and its index to disk\n",
    "dataset_path = \"dataset\"\n",
    "index_path = \"index\"\n",
    "dataset.save_to_disk(dataset_path)\n",
    "dataset.add_faiss_index(column=\"embeddings\")\n",
    "dataset.get_index(\"embeddings\").save(index_path)\n",
    "\n",
    "# Initialize the tokenizer, retriever, and model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"custom\",\n",
    "    passages_path=dataset_path,\n",
    "    index_path=index_path\n",
    ")\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "# Function to get answers using the RAG model\n",
    "def get_answer(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    outputs = model.generate(input_ids, num_beams=5, num_return_sequences=1)\n",
    "    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return answer\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"RAG-powered Chatbot\")\n",
    "st.write(\"Ask me anything about Pride and Prejudice!\")\n",
    "\n",
    "query = st.text_input(\"Your question:\")\n",
    "if query:\n",
    "    answer = get_answer(query)\n",
    "    st.write(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
